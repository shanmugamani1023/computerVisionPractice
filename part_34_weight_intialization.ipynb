{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **deep learning**, initializing weights properly is critical for training stability and faster convergence. TensorFlow/Keras provides various weight initializers, including **Xavier (Glorot) initialization** and **He initialization**, both designed to address vanishing and exploding gradient problems.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Xavier (Glorot) Initialization**\n",
    "Xavier initialization sets the weights to values that help keep the variance of activations consistent across layers. It is best suited for **sigmoid** and **tanh** activations.  \n",
    "\n",
    "The formula for Xavier initialization:\n",
    "\\[\n",
    "W \\sim \\mathcal{N}(0, \\frac{2}{n_{\\text{in}} + n_{\\text{out}}})\n",
    "\\]\n",
    "where \\(n_{\\text{in}}\\) is the number of inputs to the layer, and \\(n_{\\text{out}}\\) is the number of outputs.\n",
    "\n",
    "### **Implementation in TensorFlow/Keras:**\n",
    "```python\n",
    "from tensorflow.keras import layers, initializers\n",
    "\n",
    "# Xavier (Glorot) initialization for a Dense layer\n",
    "xavier_initializer = initializers.GlorotNormal()  # Normal distribution\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(units=512, activation='tanh', \n",
    "                 kernel_initializer=xavier_initializer, input_shape=(28, 28)),\n",
    "    layers.Dense(units=10, activation='softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "Alternatively, you can use **GlorotUniform**:\n",
    "```python\n",
    "xavier_initializer = initializers.GlorotUniform()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **2. He Initialization**\n",
    "He initialization works better with **ReLU** and **variants of ReLU** (e.g., LeakyReLU) by accounting for the fact that many ReLU units can output zero, effectively \"dropping\" neurons.  \n",
    "\n",
    "The formula for He initialization:\n",
    "\\[\n",
    "W \\sim \\mathcal{N}(0, \\frac{2}{n_{\\text{in}}})\n",
    "\\]\n",
    "where \\(n_{\\text{in}}\\) is the number of inputs to the layer.\n",
    "\n",
    "### **Implementation in TensorFlow/Keras:**\n",
    "```python\n",
    "he_initializer = initializers.HeNormal()  # Normal distribution\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(units=512, activation='relu', \n",
    "                 kernel_initializer=he_initializer, input_shape=(28, 28)),\n",
    "    layers.Dense(units=10, activation='softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "Alternatively, you can use **HeUniform**:\n",
    "```python\n",
    "he_initializer = initializers.HeUniform()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Example: Using Xavier and He Initializations in a Model**\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, initializers, Sequential\n",
    "\n",
    "# Define a model using different initializers\n",
    "model = Sequential([\n",
    "    # Layer with Xavier (Glorot) Initialization\n",
    "    layers.Dense(units=512, activation='tanh', \n",
    "                 kernel_initializer=initializers.GlorotNormal(), input_shape=(28, 28)),\n",
    "    \n",
    "    # Layer with He Initialization\n",
    "    layers.Dense(units=128, activation='relu', \n",
    "                 kernel_initializer=initializers.HeNormal()),\n",
    "    \n",
    "    # Output layer\n",
    "    layers.Dense(units=10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Summary of Initializers**\n",
    "\n",
    "| **Initializer**      | **Formula**                                 | **Best For**        |\n",
    "|----------------------|----------------------------------------------|---------------------|\n",
    "| GlorotNormal         | \\(W \\sim \\mathcal{N}(0, \\frac{2}{n_{in} + n_{out}})\\) | Sigmoid, Tanh      |\n",
    "| GlorotUniform        | \\(W \\sim U(-\\sqrt{\\frac{6}{n_{in} + n_{out}}}, \\sqrt{\\frac{6}{n_{in} + n_{out}}})\\) | Sigmoid, Tanh |\n",
    "| HeNormal             | \\(W \\sim \\mathcal{N}(0, \\frac{2}{n_{in}})\\)  | ReLU, Leaky ReLU    |\n",
    "| HeUniform            | \\(W \\sim U(-\\sqrt{\\frac{6}{n_{in}}}, \\sqrt{\\frac{6}{n_{in}}})\\) | ReLU, Leaky ReLU |\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Conclusion**\n",
    "- Use **Xavier (Glorot)** initialization for **sigmoid** and **tanh** activations.\n",
    "- Use **He** initialization for **ReLU**-based activations.\n",
    "\n",
    "These initializers ensure that the gradients neither explode nor vanish during backpropagation, allowing for stable and efficient training.\n",
    "\n",
    "Let me know if you need further help!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
