{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch Normalization (Batch Norm)** is a technique to improve the training of deep neural networks. It normalizes the output of each layer to have a mean of zero and a variance of one. This helps stabilize and accelerate training, making the model less sensitive to weight initialization and allows for higher learning rates.\n",
    "\n",
    "### **1. How Batch Normalization Works**\n",
    "- **Normalization**: For each mini-batch, the inputs are normalized.\n",
    "- **Scaling and Shifting**: After normalization, the outputs are scaled and shifted using learned parameters.\n",
    "\n",
    "### **2. Implementing Batch Normalization in TensorFlow/Keras**\n",
    "\n",
    "You can easily add batch normalization to your model using the `BatchNormalization` layer.\n",
    "\n",
    "### **Example Implementation:**\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "\n",
    "# Define the model with Batch Normalization\n",
    "inputs = Input(shape=(28, 28))\n",
    "\n",
    "# Flatten the input\n",
    "x = layers.Flatten()(inputs)\n",
    "\n",
    "# First Dense Layer\n",
    "x = layers.Dense(units=512, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)  # Apply Batch Normalization\n",
    "\n",
    "# Second Dense Layer\n",
    "x = layers.Dense(units=256, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)  # Apply Batch Normalization\n",
    "\n",
    "# Output Layer\n",
    "outputs = layers.Dense(units=10, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "### **3. Key Points**\n",
    "- **Placement**: Batch Normalization can be placed:\n",
    "  - Before the activation function (common practice).\n",
    "  - After the activation function, but this is less common.\n",
    "- **Training vs. Inference**: During training, Batch Norm uses the batch statistics (mean and variance), while during inference, it uses the moving averages computed during training.\n",
    "\n",
    "### **4. Training the Model**\n",
    "You can train your model just like any other Keras model:\n",
    "\n",
    "```python\n",
    "# Load and normalize the dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    x_train, y_train, \n",
    "    epochs=10, \n",
    "    validation_data=(x_test, y_test)\n",
    ")\n",
    "```\n",
    "\n",
    "### **5. Why Use Batch Normalization?**\n",
    "- **Stabilizes Training**: Reduces the internal covariate shift, helping to stabilize the learning process.\n",
    "- **Higher Learning Rates**: Allows for higher learning rates without risk of divergence.\n",
    "- **Acts as Regularization**: Often reduces the need for Dropout, as it introduces some noise into the training process.\n",
    "\n",
    "### **6. Batch Normalization in Convolutional Networks**\n",
    "Batch normalization is commonly used in convolutional neural networks (CNNs) as well:\n",
    "\n",
    "```python\n",
    "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(inputs)\n",
    "x = layers.BatchNormalization()(x)  # Apply Batch Normalization after Conv2D\n",
    "```\n",
    "\n",
    "### **7. Summary**\n",
    "Batch normalization is a simple yet effective technique that can greatly improve the training of deep neural networks by normalizing the inputs to each layer, allowing the network to learn faster and perform better.\n",
    "\n",
    "If you have any questions or need further clarification, feel free to ask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Certainly! Here's a complete implementation of a neural network using **Batch Normalization** in a **Sequential** model with TensorFlow/Keras. We'll use the MNIST dataset (handwritten digits) as an example.\n",
    "\n",
    "### **Implementation of Batch Normalization in a Sequential Model**\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize the data to [0, 1]\n",
    "\n",
    "# Create a Sequential model with Batch Normalization\n",
    "model = Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),          # Flatten the 28x28 images\n",
    "    layers.Dense(units=128, activation='relu'),    # First dense layer\n",
    "    layers.BatchNormalization(),                    # Batch Normalization layer\n",
    "    layers.Dense(units=64, activation='relu'),     # Second dense layer\n",
    "    layers.BatchNormalization(),                    # Another Batch Normalization layer\n",
    "    layers.Dense(units=10, activation='softmax')   # Output layer with softmax activation\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
    "```\n",
    "\n",
    "### **Explanation of the Code:**\n",
    "\n",
    "1. **Data Preparation**:\n",
    "   - The MNIST dataset is loaded and normalized to a range of [0, 1] by dividing the pixel values by 255.0.\n",
    "\n",
    "2. **Model Creation**:\n",
    "   - A **Sequential model** is created.\n",
    "   - The first layer is a **Flatten** layer, which reshapes the 28x28 images into a 784-dimensional vector.\n",
    "   - The first dense layer has 128 units and uses the **ReLU** activation function.\n",
    "   - A **BatchNormalization** layer is added right after the first dense layer to normalize the outputs of that layer.\n",
    "   - The second dense layer has 64 units and also uses ReLU activation.\n",
    "   - Another **BatchNormalization** layer is included after the second dense layer.\n",
    "   - The output layer has 10 units (one for each digit) and uses the **softmax** activation function.\n",
    "\n",
    "3. **Model Compilation**:\n",
    "   - The model is compiled using the **Adam optimizer** and the **sparse categorical cross-entropy loss function**, which is suitable for multi-class classification problems.\n",
    "\n",
    "4. **Model Training**:\n",
    "   - The model is trained for 5 epochs, with validation on the test dataset.\n",
    "\n",
    "### **Summary**\n",
    "This implementation demonstrates how to use Batch Normalization in a simple neural network with the Sequential API. Batch normalization is applied after the dense layers, helping to stabilize the training process and improve model performance. You can adjust the number of epochs, batch size, and other hyperparameters as needed for your experiments.\n",
    "\n",
    "Feel free to ask if you have any further questions or need more assistance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
