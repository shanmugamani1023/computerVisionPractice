{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks (CNNs) come in various architectures, each optimized for different goals such as accuracy, speed, efficiency, or scalability. Below are the major types of CNN architectures along with examples:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **LeNet Architecture**\n",
    "- **Developed by:** Yann LeCun (1998)  \n",
    "- **Architecture:**  \n",
    "  Conv -> Pool -> Conv -> Pool -> FC (Fully Connected)  \n",
    "- **Use Case:** Digit recognition (e.g., MNIST dataset)  \n",
    "- **Key Features:** Simple architecture with only a few layers.  \n",
    "- **Limitation:** Not scalable for complex images.  \n",
    "- **Example:** LeNet-5\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **AlexNet Architecture**\n",
    "- **Developed by:** Alex Krizhevsky et al. (2012)  \n",
    "- **Architecture:**  \n",
    "  Conv -> Pool -> Conv -> Pool -> Multiple FC -> Softmax  \n",
    "- **Use Case:** Large-scale image classification (ImageNet)  \n",
    "- **Key Features:** Introduced **ReLU** activation, **Dropout**, and **Overlapping Max Pooling** to prevent overfitting.  \n",
    "- **Limitation:** Computationally expensive.  \n",
    "- **Example:** 8-layer AlexNet\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **VGG Networks**  \n",
    "- **Developed by:** Visual Geometry Group (2014)  \n",
    "- **Architecture:**  \n",
    "  Stack of small (3x3) conv layers followed by Max Pooling and FC layers.  \n",
    "- **Use Case:** Image classification and feature extraction  \n",
    "- **Key Features:** Uniform, deep architecture (e.g., VGG-16, VGG-19) with up to 19 layers.  \n",
    "- **Limitation:** Large model size and slow training/inference.  \n",
    "- **Example:** VGG-16, VGG-19\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **ResNet (Residual Networks)**  \n",
    "- **Developed by:** Kaiming He et al. (2015)  \n",
    "- **Architecture:**  \n",
    "  Residual blocks with skip (identity) connections.  \n",
    "- **Use Case:** Deep architectures for classification and detection.  \n",
    "- **Key Features:** Solves the **vanishing gradient problem** by using skip connections. Allows networks to be extremely deep (e.g., ResNet-50, ResNet-152).  \n",
    "- **Limitation:** Increased depth adds computational complexity.  \n",
    "- **Example:** ResNet-50, ResNet-101, ResNet-152\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Inception Networks (GoogLeNet)**  \n",
    "- **Developed by:** Google (2014)  \n",
    "- **Architecture:**  \n",
    "  Multiple convolutional filter sizes (1x1, 3x3, 5x5) applied in parallel within **Inception modules**.  \n",
    "- **Use Case:** Image classification on ImageNet  \n",
    "- **Key Features:** Efficient in terms of computation and memory.  \n",
    "  - **Inception V1:** Original GoogLeNet (22 layers)  \n",
    "  - **Inception V3 & V4:** Deeper with batch normalization.  \n",
    "- **Limitation:** Complex architecture with many hyperparameters.  \n",
    "- **Example:** Inception V3, Inception-ResNet\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **MobileNets**  \n",
    "- **Developed by:** Google (2017)  \n",
    "- **Architecture:**  \n",
    "  Uses **Depthwise Separable Convolutions** to reduce computation.  \n",
    "- **Use Case:** Mobile and embedded devices  \n",
    "- **Key Features:** Lightweight and efficient; good for real-time inference on resource-limited hardware.  \n",
    "- **Limitations:** May compromise accuracy for speed.  \n",
    "- **Example:** MobileNet V1, V2, V3\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **EfficientNet**  \n",
    "- **Developed by:** Google (2019)  \n",
    "- **Architecture:**  \n",
    "  A compound scaling method that adjusts **width, depth, and resolution** systematically.  \n",
    "- **Use Case:** General-purpose classification with high accuracy and efficiency.  \n",
    "- **Key Features:** Achieves better performance with fewer parameters compared to other models.  \n",
    "- **Example:** EfficientNet-B0 to B7\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **DenseNet (Densely Connected Networks)**  \n",
    "- **Developed by:** Gao Huang et al. (2017)  \n",
    "- **Architecture:**  \n",
    "  Each layer receives inputs from all previous layers (dense connections).  \n",
    "- **Use Case:** Classification, object detection, and segmentation  \n",
    "- **Key Features:** Reduces the **vanishing gradient problem** and improves feature reuse.  \n",
    "- **Limitation:** Memory-intensive due to the dense connections.  \n",
    "- **Example:** DenseNet-121, DenseNet-169\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| **Architecture**    | **Key Feature**                                | **Example**      | **Use Case**                     |\n",
    "|---------------------|-------------------------------------------------|-----------------|----------------------------------|\n",
    "| LeNet               | Simple with few layers                         | LeNet-5          | Digit recognition (MNIST)        |\n",
    "| AlexNet             | Introduced ReLU and Dropout                    | AlexNet          | Large-scale image classification|\n",
    "| VGG                 | Deep with uniform architecture                 | VGG-16, VGG-19   | Image classification            |\n",
    "| ResNet              | Skip connections for deep networks             | ResNet-50        | Classification and detection    |\n",
    "| Inception           | Multi-scale feature extraction in parallel     | Inception V3     | ImageNet classification         |\n",
    "| MobileNet           | Lightweight with depthwise separable convolutions | MobileNet V2 | Mobile applications            |\n",
    "| EfficientNet        | Compound scaling of width, depth, and resolution | EfficientNet-B0 | High efficiency and accuracy   |\n",
    "| DenseNet            | Dense connections for feature reuse            | DenseNet-121     | Classification and segmentation |\n",
    "\n",
    "---\n",
    "\n",
    "These CNN architectures have evolved over time to tackle challenges like vanishing gradients, computational efficiency, and scalability, making them suitable for a wide range of tasks, from mobile apps to large-scale vision models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1.alex net - imagenet 2012 winner , introduce relu activation function ,computationally expensive at the time\n",
    "## 2.vgg -increase more depth ,computationally expensive\n",
    "## 3.google net (inception block) -parallel fillters -to extract more informations -computationally less expensive than previous alex,vgg\n",
    "## 4.resnet - residual block with skip connections\n",
    "## 5.efficient net - compound scaling method for use depth,width,input\n",
    "## 6.Dense net - Dense connections\n",
    "## 7.mobile-net - Depthwise convolution -suitable for edge devices ,takes less computation resources.but it less accurate.its compensate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
